# src/generation/agents/agent4_lightweight_verifier.py
# Agent 4: Lightweight Verifier (Simplified: Layer1 + Single Layer2)

"""
Agent 4: Lightweight Verifier

Agent 4 performs fast, lightweight quality checks on questions generated by Agent 3,
primarily filtering obvious format errors and extreme issues.

Position in Pipeline:
Stage 1 -> Agent 4 (Lightweight QC / Gatekeeper)

[2025-12 Adjustment Notes]
This version of Agent4 is fixed to 2 layers:
- Layer1: Structure/format hard check (rule-based, no LLM calls);
- Layer2: Single solver self-consistency check (only 1 LLM call, requires material),
  marks as extreme inconsistency only when "high confidence + evidence + inconsistent with standard answer".

No longer retains Layer3/Layer4 placeholders, no multi-round stable disagreement.

Input:
- GeneratedQuestion: Agent 3 output (single-choice or essay).
- Optional: MaterialDimSelection / SynthesizedPrompt / material text, for Layer2 solver calls.

Output:
- VerifiedQuestionSet: Contains quality score, issue list, decision labels (need_revision / is_reject), and:
  - prompt_revision_suggestion: hint_text generated from Layer1/Layer2 issues, for feedback to prompt extraction module.

Key Changes:
- Layer2 LLM input only includes: material + question (with options) + standard answer + explanation;
- No longer passes fusion prompt P, ability_point, anchors, etc. to LLM.

Note on metadata:
- metadata/tags in this file are NOT input to LLM, only for logging/tracking:
  - prompt_logger.save_agent_log(..., metadata=...): Written to local logs, for locating which question/type a call corresponds to;
  - verifier_tags: Written to VerifiedQuestionSet result, for later statistics and debugging.
- If you only care about final question and score, you can reduce, but suggest keeping at least question_type, num_issues, etc.
"""

from typing import List, Optional, Dict, Any
import re
import json

from src.shared.schemas import (
    GeneratedQuestion,
    VerifiedQuestionSet,
    LightweightQualityScore,
    QualityIssue,
    MaterialDimSelection,
)
from src.shared.config import Agent4Config
from src.shared.llm_interface import LLMClient
from src.shared.prompt_logger import PromptLogger


# ============================================================================
# Helper functions
# ============================================================================

def to_float01(x: Any) -> Optional[float]:
    """
    Safely convert a value to a float in [0, 1]. Return None if invalid.
    """
    if x is None:
        return None
    try:
        v = float(x)
        if v != v:  # NaN
            return None
        if v < 0.0:
            v = 0.0
        if v > 1.0:
            v = 1.0
        return v
    except (ValueError, TypeError):
        return None


def parse_solver_response(text: str) -> Dict[str, Any]:
    """
    Parse solver JSON response.

    Expected (minimum) keys:
      - answer: "A/B/C/D" or short text
      - confidence: 0..1
      - evidence: [...]
    Optional keys:
      - verdict: "consistent" | "inconsistent" | "uncertain"
      - notes / rationale: string

    Prefer ```json ...``` if exists; else take outermost {...}
    Never crash.
    """
    raw = (text or "").strip()

    m = re.search(r"```json\s*(\{.*?\})\s*```", raw, flags=re.S)
    if m:
        cand = m.group(1).strip()
    else:
        left = raw.find("{")
        right = raw.rfind("}")
        cand = raw[left:right + 1].strip() if (left != -1 and right != -1 and right > left) else ""

    if not cand:
        return {"answer": None, "confidence": None, "evidence": [], "verdict": None, "notes": None, "revision_suggestion": None, "parse_failed": True}

    try:
        obj = json.loads(cand)
    except (json.JSONDecodeError, ValueError):
        return {"answer": None, "confidence": None, "evidence": [], "verdict": None, "notes": None, "revision_suggestion": None, "parse_failed": True}

    ev = obj.get("evidence") or []
    if not isinstance(ev, list):
        ev = []

    return {
        "answer": obj.get("answer"),
        "confidence": obj.get("confidence"),
        "evidence": ev,
        "verdict": obj.get("verdict"),
        "notes": obj.get("notes") or obj.get("rationale"),
        "revision_suggestion": obj.get("revision_suggestion"),
        "parse_failed": False,
    }


class Agent4LightweightVerifier:
    """
    Agent 4: Lightweight Verifier (Layer1 + Single Layer2)

    Functions:
    - Layer1: Structure/format hard check (required for all questions, no LLM calls);
    - Layer2: Single consistency/self-consistency check (only when material available, only 1 LLM call).
      - LLM input only includes: material + question + standard answer + explanation (no fusion prompt/anchor info).
    """

    def __init__(
        self,
        config: Agent4Config,
        llm_client: LLMClient,
        prompt_logger: PromptLogger,
    ):
        self.config = config
        self.llm_client = llm_client
        self.prompt_logger = prompt_logger

    def run(
        self,
        agent3_output: GeneratedQuestion,
        agent1_output: Optional[MaterialDimSelection] = None,
        material_text: Optional[str] = None,
    ) -> VerifiedQuestionSet:
        """
        Fixed gatekeeper strategy: Layer1 + Single Layer2.
        - Layer1: Only examines the question itself;
        - Layer2: Requires material_text (can be obtained from agent1_output).
        """
        print("[Agent 4] Starting lightweight quality check...")

        question = agent3_output

        # Ensure material availability: prefer explicit input, otherwise get from Agent1 output
        if material_text is None and agent1_output is not None:
            material_text = agent1_output.material_text

        issues: List[QualityIssue] = []
        layer_scores: Dict[str, float] = {}

        # ========== Layer 1: Structure & Format Completeness Check (required, no LLM) ==========
        self._check_format(question, issues)
        layer_scores["layer1_format"] = self._score_layer(issues, layer_name="layer1_format")

        # ========== Layer 2: Single Consistency Check (one round only, requires material) ==========
        consistency_result: Optional[Dict[str, Any]] = None
        l2_enabled_flag = getattr(self.config, "l2_stable_disagreement_count", 1)  # >0 means enable one check

        if l2_enabled_flag <= 0:
            layer_scores["layer2_consistency"] = 1.0
            print("  [Layer2] Skipped (config l2_stable_disagreement_count <= 0, Layer2 disabled)")
        elif material_text:
            consistency_result = self._check_consistency_single_shot(
                material_text=material_text,
                question=question,
            )
            if consistency_result.get("trigger_extreme", False):
                issues.append(
                    QualityIssue(
                        layer="layer2_consistency",
                        issue_type="extreme_inconsistency",
                        description=consistency_result.get(
                            "note",
                            "Extreme inconsistency: Single high-confidence + evidence review concludes standard answer/explanation conflicts with material.",
                        ),
                    )
                )
            layer_scores["layer2_consistency"] = self._score_layer(issues, layer_name="layer2_consistency")
        else:
            layer_scores["layer2_consistency"] = 1.0
            print("  [Layer2] Skipped (missing material_text)")

        # ========== Aggregate Scores ==========
        overall_score = self._aggregate_scores(layer_scores)

        # Gatekeeper decision: Any Layer1 issue OR Layer2 extreme inconsistency -> need_revision
        has_layer1_issue = any(i.layer == "layer1_format" for i in issues)
        has_extreme_inconsistency = any(
            i.layer == "layer2_consistency" and i.issue_type == "extreme_inconsistency"
            for i in issues
        )
        need_revision = has_layer1_issue or has_extreme_inconsistency

        # Rejection: Keep simple threshold (avoid false positives)
        reject_threshold = getattr(self.config, "reject_threshold", 0.3)
        is_reject = overall_score < reject_threshold

        quality_score = LightweightQualityScore(
            overall_score=overall_score,
            layer_scores=layer_scores,
        )

        verifier_tags = self._build_verifier_tags(
            need_revision=need_revision,
            is_reject=is_reject,
            issues=issues,
        )

        # ========== Build prompt_revision_suggestion from Layer1/Layer2 Issues ==========
        prompt_revision_suggestion: Optional[str] = None
        if need_revision:
            prompt_revision_suggestion = self._build_prompt_revision_suggestion(
                issues=issues,
                consistency_result=consistency_result,
                question=question,  # [2025-12-29 Added] Pass question for outputting previous round content
            )

        verified = VerifiedQuestionSet(
            original_question=question,
            quality_score=quality_score,
            issues=issues,
            need_revision=need_revision,
            is_reject=is_reject,
            verifier_tags=verifier_tags,
        )
        # For next round P revision (can remove from schema or not assign here if not needed)
        verified.prompt_revision_suggestion = prompt_revision_suggestion

        print(f"[Agent 4] [OK] Quality check complete")
        print(f"  - overall_score = {overall_score:.2f}")
        print(f"  - need_revision = {need_revision}, is_reject = {is_reject}")
        if prompt_revision_suggestion:
            print("  - Generated prompt_revision_suggestion (for prompt module reference)")

        return verified

    # ========== Layer 1: Format / Structure ==========

    def _check_format(self, question: GeneratedQuestion, issues: List[QualityIssue]) -> None:
        """
        Layer 1: Structure & format completeness check (gatekeeper hard check).

        Note (relaxed as required):
        - Essay total_score doesn't require fixed value (6/8/10 all acceptable), only requires "positive integer".
        """
        # 1) Stem check
        if not question.stem or not str(question.stem).strip():
            issues.append(
                QualityIssue(
                    layer="layer1_format",
                    issue_type="empty_stem",
                    description="Stem is empty or contains only whitespace.",
                )
            )

        # 2) Single-choice structure check
        if question.question_type == "single-choice":
            if not question.options or len(question.options) < 4:
                issues.append(
                    QualityIssue(
                        layer="layer1_format",
                        issue_type="insufficient_options",
                        description="Single-choice question has fewer than 4 options.",
                    )
                )
            labels = {opt.label for opt in (question.options or [])}
            if question.correct_answer not in labels:
                issues.append(
                    QualityIssue(
                        layer="layer1_format",
                        issue_type="correct_answer_not_in_options",
                        description="Correct answer is not in the option label set.",
                    )
                )

        # 3) Essay structure check
        if question.question_type == "essay":
            if not question.answer_points:
                issues.append(
                    QualityIssue(
                        layer="layer1_format",
                        issue_type="empty_answer_points",
                        description="Essay question has no answer points.",
                    )
                )

            # total_score: Only requires positive integer, no size limit
            if question.total_score is None or not isinstance(question.total_score, int) or question.total_score <= 0:
                issues.append(
                    QualityIssue(
                        layer="layer1_format",
                        issue_type="invalid_total_score",
                        description="Essay total_score is not set or is not a positive integer (accepts any positive integer like 6/8/10).",
                    )
                )
            else:
                # Optional hint: Point sum differs too much from total_score, only hint without hard rejection
                if question.answer_points:
                    try:
                        s = sum(int(p.score or 0) for p in question.answer_points)
                        if s > 0 and abs(question.total_score - s) >= max(3, int(0.5 * s)):
                            issues.append(
                                QualityIssue(
                                    layer="layer1_format",
                                    issue_type="total_score_mismatch_hint",
                                    description=(
                                        f"Hint: total_score={question.total_score} differs significantly from answer points sum={s} (not a hard rejection)."
                                    ),
                                )
                            )
                    except Exception:
                        pass

        # 4) Explanation check (gatekeeper: missing explanation is obvious garbage)
        if not question.explanation or not str(question.explanation).strip():
            issues.append(
                QualityIssue(
                    layer="layer1_format",
                    issue_type="missing_explanation",
                    description="Missing explanation.",
                )
            )

    # ========== Layer 2: Single-shot consistency/audit gate ==========

    def _check_consistency_single_shot(
        self,
        material_text: str,
        question: GeneratedQuestion,
    ) -> Dict[str, Any]:
        """
        Layer 2: Single consistency/audit check (only 1 LLM call).

        Input: Material + question (with options) + standard answer + explanation (no fusion prompt/anchors, etc.).
        Output: LLM returns verdict (consistent/inconsistent/uncertain), suggested answer, confidence, evidence.

        [2025-12-29 Update] Extended trigger conditions:
        1. All types: verdict=inconsistent/uncertain + conf>=threshold -> trigger
        2. Single-choice only: predicted != gold + conf>=threshold -> trigger (even if verdict=consistent)

        [2026-01 Update] Confidence threshold adjusted from 0.8 to 0.6
        """
        confidence_threshold = getattr(self.config, "l2_confidence_threshold", 0.6)

        solve_result = self.llm_solve_question(
            material_text=material_text,
            question=question,
        )

        predicted = solve_result.get("predicted_answer")
        verdict = (solve_result.get("verdict") or "").strip().lower() or None
        raw_conf = solve_result.get("llm_confidence")
        evidence = solve_result.get("evidence") or []
        parse_failed = solve_result.get("parse_failed", False)

        gold_answer = question.correct_answer
        conf_safe = to_float01(raw_conf)
        trigger_extreme = False
        trigger_reason = ""  # Record trigger reason

        # [2025-12-29 Update] Extended to all question types
        if not parse_failed and conf_safe is not None:
            # Condition 1: All types - verdict is inconsistent or uncertain + conf>=threshold(0.6)
            if (verdict == "inconsistent" or verdict == "uncertain") and conf_safe >= confidence_threshold:
                trigger_extreme = True
                trigger_reason = f"verdict={verdict}, conf={conf_safe:.2f}>=threshold{confidence_threshold}"

            # Condition 2: Single-choice only - answer mismatch + conf>=threshold(0.6) (even if verdict=consistent)
            if question.question_type == "single-choice":
                if predicted and gold_answer and predicted.strip().upper() != gold_answer.strip().upper():
                    if conf_safe >= confidence_threshold:
                        trigger_extreme = True
                        trigger_reason = f"Single-choice answer mismatch: LLM suggested={predicted} vs standard={gold_answer}, conf={conf_safe:.2f}"

        if trigger_extreme:
            note = (
                f"Triggering iteration: {trigger_reason}. "
                f"[Details] verdict={verdict}, suggested answer={predicted}, standard answer={gold_answer}, "
                f"conf={conf_safe:.2f}, evidence count={len(evidence)}."
            )
            print(f"  [Layer2] [!] {note}")
        else:
            note = solve_result.get("consistency_note", "Did not trigger iteration gate.")
            print(f"  [Layer2] [OK] {note}")

        diagnostics = {
            "gold_answer": gold_answer,
            "predicted_answer": predicted,
            "verdict": verdict,
            "confidence": conf_safe,
            "evidence_count": len(evidence),
            "parse_failed": parse_failed,
            "confidence_threshold": confidence_threshold,
            "trigger_reason": trigger_reason if trigger_extreme else None,
            "question_type": question.question_type,
        }

        return {
            "trigger_extreme": trigger_extreme,
            "trigger_reason": trigger_reason,
            "note": note,
            "solver_result": solve_result,
            "diagnostics": diagnostics,
        }

    # ========== Scoring / Tags / Hint Text ==========

    def _score_layer(self, issues: List[QualityIssue], layer_name: str) -> float:
        """Calculate score for a specific layer."""
        layer_issues = [i for i in issues if i.layer == layer_name]
        if not layer_issues:
            return 1.0
        n = len(layer_issues)
        if n == 1:
            return 0.7
        if n == 2:
            return 0.4
        return 0.2

    def _aggregate_scores(self, layer_scores: Dict[str, float]) -> float:
        """Aggregate layer scores into overall score."""
        if not layer_scores:
            return 0.0
        return sum(layer_scores.values()) / len(layer_scores)

    def _build_verifier_tags(
        self,
        need_revision: bool,
        is_reject: bool,
        issues: List[QualityIssue],
    ) -> Dict[str, Any]:
        """Build verifier tags for output metadata."""
        # These tags are NOT input to LLM, only "metadata/statistics" in output
        tags: Dict[str, Any] = {
            "need_revision": need_revision,
            "is_reject": is_reject,
            "num_issues": len(issues),
        }
        layer_counts: Dict[str, int] = {}
        for issue in issues:
            layer_counts[issue.layer] = layer_counts.get(issue.layer, 0) + 1
        tags["layer_issue_counts"] = layer_counts
        tags["main_issue_types"] = list({issue.issue_type for issue in issues})
        return tags

    def _build_prompt_revision_suggestion(
        self,
        issues: List[QualityIssue],
        consistency_result: Optional[Dict[str, Any]] = None,
        question: Optional[GeneratedQuestion] = None,  # [2025-12-29 Added] Previous round question
    ) -> Optional[str]:
        """
        Generate revision suggestions for next iteration based on Layer1/Layer2 issues.
        Only called when need_revision=True.

        [2025-12-29 Enhancement]
        1. Includes LLM's specific revision suggestions, evidence references, etc.
        2. Includes previous round's complete question content (stem, options, answer, explanation)
        3. Provides specific revision guidance for different format issues
        """
        if not issues:
            return None

        l1_msgs: List[str] = []
        l2_msgs: List[str] = []

        for issue in issues:
            if issue.layer == "layer1_format":
                if issue.issue_type == "empty_stem":
                    l1_msgs.append("[Empty Stem] Stem is empty or too brief, please provide complete, clear stem expression that can be understood independently.")
                elif issue.issue_type == "insufficient_options":
                    l1_msgs.append("[Insufficient Options] Single-choice question has fewer than 4 options, please complete A/B/C/D with substantive content.")
                elif issue.issue_type == "correct_answer_not_in_options":
                    l1_msgs.append("[Wrong Answer] Standard answer label is not in option set, please ensure correct_answer field value (e.g., A/B/C/D) matches actual option labels.")
                elif issue.issue_type == "missing_explanation":
                    l1_msgs.append("[Missing Explanation] Missing explanation, please add material-based explanation text describing why this answer is chosen.")
                elif issue.issue_type == "empty_answer_points":
                    l1_msgs.append("[Empty Answer Points] Essay question lacks answer points, please split into several scorable points (answer_points), each with content and score.")
                elif issue.issue_type == "invalid_total_score":
                    l1_msgs.append("[Invalid Total Score] Essay total_score is not set or invalid, please set to a reasonable positive integer (e.g., 6/8/10).")
                elif issue.issue_type == "total_score_mismatch_hint":
                    l1_msgs.append("[Score Mismatch] Essay total_score differs significantly from answer points sum, please check if score design is reasonable.")
                else:
                    l1_msgs.append(f"[Format Issue] {issue.description or 'Has format/structure issues, please check stem, options, answer points and explanation.'}")

            elif issue.layer == "layer2_consistency":
                if issue.issue_type == "extreme_inconsistency":
                    l2_msgs.append("[Content Inconsistency] Material-based review concludes standard answer/explanation conflicts with material, please re-check if answer and explanation can be directly supported by material.")

        # [Enhancement] Extract detailed info from consistency_result returned by LLM
        llm_revision_suggestion = None
        llm_evidence = []
        llm_notes = None
        predicted_vs_gold = None

        if consistency_result is not None:
            solver_result = consistency_result.get("solver_result", {})
            diagnostics = consistency_result.get("diagnostics", {})

            # Extract specific revision suggestions from LLM
            llm_revision_suggestion = solver_result.get("revision_suggestion")
            llm_evidence = solver_result.get("evidence") or []
            llm_notes = consistency_result.get("note")

            # Extract answer comparison info
            gold = diagnostics.get("gold_answer")
            predicted = diagnostics.get("predicted_answer")
            if gold and predicted:
                predicted_vs_gold = f"QC model suggested answer: {predicted}, Current generated standard answer: {gold}"

        parts: List[str] = []

        # Layer1 format issues
        if l1_msgs:
            parts.append("[Structure/Format Issues - Must Fix]\n" + "\n".join(f"  - {m}" for m in sorted(set(l1_msgs))))

        # Layer2 consistency issues
        if l2_msgs or llm_revision_suggestion or llm_evidence:
            l2_content = "[Consistency Issues - Need Fix]\n"
            if l2_msgs:
                l2_content += "\n".join(f"  - {m}" for m in sorted(set(l2_msgs))) + "\n"

            # Add answer comparison
            if predicted_vs_gold:
                l2_content += f"  - {predicted_vs_gold}\n"

            # Add LLM's specific revision suggestion (most important)
            if llm_revision_suggestion:
                l2_content += f"  - [Revision Suggestion] {llm_revision_suggestion}\n"

            # Add material evidence cited by LLM
            if llm_evidence:
                l2_content += "  - [Material Evidence]\n"
                for i, ev in enumerate(llm_evidence[:3], 1):  # Max 3
                    l2_content += f"    {i}. \"{ev[:150]}{'...' if len(ev) > 150 else ''}\"\n"

            # Add LLM notes
            if llm_notes:
                l2_content += f"  - [Issue Details] {llm_notes}\n"

            parts.append(l2_content.rstrip())

        # [2025-12-29 Added] Add previous round's question content
        if question is not None:
            prev_output_parts = ["[Previous Round Generated Question - Please Fix Based on This]"]

            # Stem
            stem = getattr(question, "stem", None)
            if stem:
                prev_output_parts.append(f"  Stem: {stem[:500]}{'...' if len(str(stem)) > 500 else ''}")

            # Question type
            qtype = getattr(question, "question_type", None)
            prev_output_parts.append(f"  Question type: {qtype or 'unknown'}")

            # Single-choice options
            if qtype == "single-choice":
                options = getattr(question, "options", None) or []
                if options:
                    prev_output_parts.append("  Options:")
                    for opt in options:
                        label = getattr(opt, "label", "?")
                        content = getattr(opt, "content", "")
                        prev_output_parts.append(f"    {label}. {content[:100]}{'...' if len(str(content)) > 100 else ''}")

                correct_answer = getattr(question, "correct_answer", None)
                prev_output_parts.append(f"  Standard answer: {correct_answer or 'not set'}")

            # Essay answer points
            elif qtype == "essay":
                answer_points = getattr(question, "answer_points", None) or []
                total_score = getattr(question, "total_score", None)
                prev_output_parts.append(f"  Total score: {total_score or 'not set'}")
                if answer_points:
                    prev_output_parts.append(f"  Answer points ({len(answer_points)}):")
                    for i, pt in enumerate(answer_points[:5], 1):
                        content = getattr(pt, "content", "")
                        score = getattr(pt, "score", 0)
                        prev_output_parts.append(f"    {i}. [{score}pts] {content[:80]}{'...' if len(str(content)) > 80 else ''}")
                    if len(answer_points) > 5:
                        prev_output_parts.append(f"    ... (total {len(answer_points)} points)")

            # Explanation
            explanation = getattr(question, "explanation", None)
            if explanation:
                prev_output_parts.append(f"  Explanation: {explanation[:300]}{'...' if len(str(explanation)) > 300 else ''}")
            else:
                prev_output_parts.append("  Explanation: (missing)")

            parts.append("\n".join(prev_output_parts))

        if not parts:
            return None

        header = (
            "=" * 60 + "\n"
            "[Iteration Revision Guide] Previous round QC found following issues, please fix in this round:\n"
            "=" * 60 + "\n"
        )
        body = "\n".join(parts)
        footer = "\n" + "=" * 50
        return f"{header}{body}{footer}"

    # ========== LLM Audit / Solver (for Layer2) ==========

    def llm_solve_question(
        self,
        material_text: str,
        question: GeneratedQuestion,
    ) -> Dict[str, Any]:
        """
        Call LLM to solve/audit a question based on material.
        """
        solver_prompt = self._build_solver_prompt(
            material_text=material_text,
            question=question,
        )

        messages = [{"role": "user", "content": solver_prompt}]
        response = self.llm_client.generate(messages)

        # metadata: For logging only (NOT input to LLM)
        self.prompt_logger.save_agent_log(
            agent_name="Agent4_Solver",
            stage="consistency_check",
            prompt=solver_prompt,
            response=response,
            metadata={
                "question_type": question.question_type,
            },
            model=self.llm_client.model_name,
        )

        raw_response = (response or "").strip()
        parsed = parse_solver_response(raw_response)

        predicted_answer = parsed.get("answer")
        llm_confidence = parsed.get("confidence")
        evidence = parsed.get("evidence") or []
        verdict = parsed.get("verdict")
        notes = parsed.get("notes")
        revision_suggestion = parsed.get("revision_suggestion")
        parse_failed = parsed.get("parse_failed", False)

        # Parse failed fallback: For single-choice, try to extract letter (still "better to pass than falsely reject")
        if parse_failed and question.question_type == "single-choice":
            m = re.search(r"\b([ABCD])\b", raw_response)
            if m:
                predicted_answer = m.group(1)

        gold = question.correct_answer

        # Prefer verdict (if LLM provided); otherwise fallback to predicted vs gold
        is_consistent: Optional[bool] = None
        if isinstance(verdict, str):
            v = verdict.strip().lower()
            if v == "consistent":
                is_consistent = True
            elif v == "inconsistent":
                is_consistent = False

        if is_consistent is None and question.question_type == "single-choice":
            if predicted_answer and predicted_answer in "ABCD" and gold in "ABCD":
                is_consistent = (predicted_answer == gold)

        if is_consistent is True:
            consistency_note = "LLM audit concludes standard answer/explanation is consistent with material."
        elif is_consistent is False:
            consistency_note = "LLM audit concludes standard answer/explanation is inconsistent with material."
        else:
            consistency_note = "LLM audit conclusion uncertain/parse failed, not making hard judgment."

        if notes:
            consistency_note = f"{consistency_note} Notes: {notes}"

        return {
            "predicted_answer": predicted_answer,
            "verdict": verdict,
            "llm_confidence": llm_confidence,
            "evidence": evidence,
            "parse_failed": parse_failed,
            "is_consistent": is_consistent,
            "consistency_note": consistency_note,
            "revision_suggestion": revision_suggestion,
            "raw_response": raw_response,
        }

    def _build_solver_prompt(
        self,
        material_text: str,
        question: GeneratedQuestion,
    ) -> str:
        """
        Layer2 LLM input (only essential content as required):
        - Material
        - Question (stem + options/essay requirements)
        - Standard answer
        - Explanation

        Not included: Fusion prompt P, ability_point, anchors, and other generation-stage information.
        """
        # Chinese JSON format instruction - kept for LLM interaction (domain-specific Gaokao evaluation)
        json_format_instruction = """

你必须仅输出严格的 JSON（不要使用 markdown 代码块、不要添加任何解释性文字）：
{
  "verdict": "consistent / inconsistent / uncertain（三选一）",
  "answer": "你基于材料给出的建议答案（单选题填 A/B/C/D；简答题填简要答案）",
  "confidence": 0.0到1.0之间的数字（表示你对 verdict 的确信程度）,
  "evidence": ["从材料中引用的支持你 verdict 的原文片段1", "原文片段2", ...],
  "notes": "一句话说明不一致点/不确定原因",
  "revision_suggestion": "如果verdict为inconsistent，请给出具体的修订建议：1）指出题目/选项/答案/解析中的具体问题；2）说明如何修正才能与材料一致；3）如果是选项设计问题，建议如何调整干扰项"
}
"""

        gold_part = ""
        if question.correct_answer:
            gold_part += f"【标准答案】{question.correct_answer}\n"
        if question.explanation:
            gold_part += f"【解析】{question.explanation}\n"

        if question.question_type == "single-choice":
            options_str = "\n".join([f"{opt.label}. {opt.content}" for opt in (question.options or [])])
            # Chinese prompt for Gaokao evaluation - kept for domain-specific LLM interaction
            return (
                "你是一名高考语文阅卷/命题审阅者。请仅依据【材料】判断【标准答案】与【解析】是否可被材料支撑。\n"
                "如你认为不一致，请给出你认为更合理的选项，并引用材料原文作为证据。\n\n"
                "【材料】\n"
                f"{material_text}\n\n"
                "【题目】\n"
                f"{question.stem}\n\n"
                "【选项】\n"
                f"{options_str}\n\n"
                f"{gold_part}\n"
                f"{json_format_instruction}"
            )

        # Essay: Audit whether answer_points/explanation can be supported by material
        # [2025-12 Fix] Essay uses answer_points instead of correct_answer
        answer_points_part = ""
        if question.answer_points:
            points_lines = []
            for i, pt in enumerate(question.answer_points, 1):
                point_text = getattr(pt, "point", "") or (pt.get("point", "") if isinstance(pt, dict) else "")
                point_score = getattr(pt, "score", "") or (pt.get("score", "") if isinstance(pt, dict) else "")
                evidence_ref = getattr(pt, "evidence_reference", "") or (pt.get("evidence_reference", "") if isinstance(pt, dict) else "")
                line = f"  {i}. {point_text}"
                if point_score:
                    line += f"({point_score}pts)"
                if evidence_ref:
                    line += f"\n     Material reference: {evidence_ref}"
                points_lines.append(line)
            answer_points_part = "【答案要点】\n" + "\n".join(points_lines) + "\n"

        total_score_part = ""
        if question.total_score:
            total_score_part = f"【总分】{question.total_score}pts\n"

        explanation_part = ""
        if question.explanation:
            explanation_part = f"【解析】{question.explanation}\n"

        # Chinese prompt for Gaokao evaluation - kept for domain-specific LLM interaction
        return (
            "你是一名高考语文阅卷/命题审阅者。请仅依据【材料】判断【答案要点】与【解析】是否可被材料支撑。\n"
            "如你认为不一致，请指出哪些答案要点与材料不符，并引用材料原文作为证据。\n\n"
            "【材料】\n"
            f"{material_text}\n\n"
            "【题目】\n"
            f"{question.stem}\n\n"
            f"{answer_points_part}"
            f"{total_score_part}"
            f"{explanation_part}\n"
            f"{json_format_instruction}"
        )


__all__ = ["Agent4LightweightVerifier"]
